{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLhZb18MNMmJ"
   },
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "In this application, we will continue to work on the Fashion MNIST dataset and will try to figure out how are CNNs used with the image data.\n",
    "\n",
    "Note that we will create a very simple model here and will only talk about the architecture with limited features. Please feel free to add/modify the architecture as per your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93SFZcjkbfvG"
   },
   "source": [
    "### Libraries, data preprocessing, and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCiWeSTwNKAY"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNX88_U46LG5",
    "outputId": "29002488-c97e-409e-9a11-7841ba99325a"
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "(X_train, trainY), (X_test,testY) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train.shape, trainY.shape, X_test.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "IsafUyVpcmuC",
    "outputId": "c6b3d941-13cc-4b01-86a9-0e408b901c9c"
   },
   "outputs": [],
   "source": [
    "# let's visualize the dataset at hand\n",
    "import matplotlib.pyplot as plt\n",
    "class_names_list = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for i in range(24):\n",
    "    plt.subplot(4,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names_list[trainY[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrxzlcMHfvbf"
   },
   "outputs": [],
   "source": [
    "# Reshaping the training and the testing data. \n",
    "# This needs to be reshaped because of the input shape requirement of the CNN\n",
    "\n",
    "x_train = X_train.reshape(60000,28,28,1)\n",
    "x_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0LlT0GPcn0z"
   },
   "outputs": [],
   "source": [
    "#let's normalize the dataset. Since there are pixel values ranging from 0-255, lets divide by 255 to get the new ranges from 0-1\n",
    "\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "\n",
    "# Also we will create one hot encoded vectors for the target variable\n",
    "y_train = tf.keras.utils.to_categorical(trainY)\n",
    "y_test = tf.keras.utils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bHWAAiYbsYf"
   },
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_XArNaEdR8y"
   },
   "outputs": [],
   "source": [
    "# Imporing the libraries as required. Note that we are using tensorflow.keras here.\n",
    "import tensorflow as tf\n",
    "# Keras Sequential Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importing all the different layers and optimizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5o1I0gPnbOg"
   },
   "source": [
    "## Model-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDmsCu2jbM9O"
   },
   "source": [
    "### Question 1 : In CNN last few layers are fully connected layers. So for model1 after flatten add 1 hidden layer with 64 neurons, activation function as relu, and an output layer. Then build the model and the test accuracy of the model lies in the range,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0lVtHwCbMJ8"
   },
   "outputs": [],
   "source": [
    "# Initiating the sequential model object\n",
    "model1 = Sequential()\n",
    "\n",
    "# Generating the model and adding layers\n",
    "# Two convolution layers and pooling layer\n",
    "model1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_last', input_shape=(28,28,1)))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the model and making room for the fully connected component\n",
    "model1.add(Flatten())\n",
    "___________________________________# write Code for hidden layer \n",
    "___________________________________# write code for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: So how many parameters does the Max pooling layer take? (check model summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNRZ-452dyUA",
    "outputId": "4842efde-8533-4b89-cbf5-7996fa8a4726"
   },
   "outputs": [],
   "source": [
    "# Let us summarize the model structure\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDkcaPGMbxGm"
   },
   "source": [
    "### Compiling, fitting and evaluating the test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUX_b6cef8gv"
   },
   "outputs": [],
   "source": [
    "# Optimizer for the model, Adam with a learning rate(lr) of 0.01.\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model with the optimizer, loss (categorical_crossentropy) and the metrics (accuracy) to look at.\n",
    "model1.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a62gI5_JgF-1",
    "outputId": "b1a18e8e-ead0-42de-9740-893e9173766a"
   },
   "outputs": [],
   "source": [
    "# Fitting the model with a batch size of 128 and 10 epochs.\n",
    "model1.fit(x_train, y_train, validation_data=(x_test, y_test), verbose = 1, batch_size=128, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwnniVKd9Z9n"
   },
   "source": [
    "### Evaluate the model on the test set\n",
    "\n",
    "- Let's predict using the test data. \n",
    "- The .predict() method in Keras models returns the probabilities of each observation belonging to each class. We will choose the class where the predicted probability is the highest.\n",
    "- Also, let's build a function to print the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14R6jUbP9aSs"
   },
   "outputs": [],
   "source": [
    "def metrics_score(actual, predicted):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(classification_report(actual, predicted))\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels=class_names_list, yticklabels=class_names_list)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "MlSEC-WgcOHH",
    "outputId": "85d6a7c4-b4eb-40ed-fdd9-13f6e7ead756"
   },
   "outputs": [],
   "source": [
    "test_pred1 = np.argmax(model1.predict(x_test), axis=-1)\n",
    "\n",
    "metrics_score(testY, test_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ_k8mDing6l"
   },
   "source": [
    "## Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3:</b> In model1 just you have only one convolution layer, to increase the performance build model2 with 2 convolution layers.One layer has been already implemented for you just add another layer with number of filters,kernel_size,activation and padding same as previous layers.\n",
    "- Note: Do not pass the input_shape parameter for other convolution layers.It should only be used in the first convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03V7vj3lnlQV"
   },
   "outputs": [],
   "source": [
    "# Initiating the sequential model object\n",
    "model2 = Sequential()\n",
    "\n",
    "# Generating the model and adding layers\n",
    "# Two convolution layers and pooling layer\n",
    "model2.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_last', input_shape=(28,28,1)))\n",
    "______________________________________________# Add convolutional layer also not pass the input_shape parameter in this layer\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the model and making room for the fully connected component\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation='relu')\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(_______, activation='________')) # Add output layer details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x9WxecBnlbj",
    "outputId": "44627f07-5e6e-49a8-bcda-68ae076492ac"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_Or82o67sgH"
   },
   "source": [
    "- The model structure has:\n",
    "  - 2 convolution layers with 16 filters each of size (3,3) and 'same' padding i.e. the output would be of same size as the input except the depth which would be equal to 16.\n",
    "  - Max pooling of size (2,2) is applied on the output of the second layer.\n",
    "  - The output of max pooled images is flattened using the Flatten layer.\n",
    "  - A Dense layer is used to create a fully connected layer with 64 nodes has been applied to the output of the Flatten layer\n",
    "  - A batch normalization layer to normalize each batch while training the model i.e. centering and scaling each batch separately. It helps to make the training process faster, reduce overfitting so that the model can generalize appropriately.\n",
    "  - Again, a Dense layer with 16 nodes and a batch normalization layer\n",
    "  - Finally, the softmax layer with 10 nodes to get the probabilities of each observation belonging to each class.\n",
    "- Each node in the batch normalization layer has two non-trainable parameters - mean and variance. Here, we have 2 batch normalization layers with 64 and 16 nodes. So, the model has (64+16)*2 = 160 non-trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rt5UdQGtnllg"
   },
   "outputs": [],
   "source": [
    "# Optimizer for the model, Adam with a learning rate(lr) of 0.01.\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model with the optimizer, loss (categorical_crossentropy) and the metrics (accuracy) to look at.\n",
    "model2.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8IeefTgoexG",
    "outputId": "a9375217-53e3-40ed-be53-b7f4081838ef"
   },
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), verbose = 1, batch_size=128, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "VBoC2yIz-jav",
    "outputId": "51509d6f-9abd-49f4-efc2-5808b5b9992c"
   },
   "outputs": [],
   "source": [
    "test_pred2 = np.argmax(model2.predict(x_test), axis=-1)\n",
    "\n",
    "metrics_score(testY, test_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73jjDZeSwHdW"
   },
   "source": [
    "- There is a hint of overfit in the modelling process. We can use regularization, dropout, batch normalization, and data augmentation to handle it. Feel free to apply these techniques on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4KL18sS312e"
   },
   "source": [
    "Let's visualize the images from the test data. \n",
    "- We will randomly select 24 images from the test data and visualize them.\n",
    "- The title of each image would show the actual and predicted label of that image and the probability of the predicted class. \n",
    "- Higher the probability more confident the model is about the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "2FOW5QWo96uY",
    "outputId": "680ea6a3-d8a4-4d8e-9d99-69ba07f8a4a0"
   },
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 6\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(testY))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.imshow(X_test[random_index, :])\n",
    "        pred_label = class_names_list[test_pred2[random_index]]\n",
    "        true_label = class_names_list[testY[random_index]]\n",
    "        y_pred_test_max_probas = np.max(model2.predict(x_test), axis=-1)\n",
    "        pred_proba = y_pred_test_max_probas[random_index]\n",
    "        ax.set_title(\"actual: {}\\npredicted: {}\\nprobability: {:.3}\\n\".format(\n",
    "               true_label, pred_label, pred_proba\n",
    "        ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCAOVC6jEg5_"
   },
   "source": [
    "### Comments\n",
    " - To avoid the overfitting, we can try out dropout layers.\n",
    "- Note the shape of the training and the testing data for the fitting process. Data should be properly shaped when sending in volumes in the first layer.\n",
    "- To get better accuracies, one can try to add more convolutional filters, more convolutional and dense layers. However, adding more nodes also increases the time required to fit the model and hence there is a trade-off.\n",
    " - In such case, batch normalization comes in handy. It can speed up the training process.\n",
    "\n",
    " **Recommendations**\n",
    " - Play around with hyperparameters and see if you can improve the accuracy.\n",
    " - Try out different optimizers and assess their effect.\n",
    " - Also, try out the initializers to check their effect on model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LVC_2_Fashion_MNIST_using_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
